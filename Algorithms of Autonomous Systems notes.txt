

Chapter 2:

1. Greedy algorithms are useful when variance is 0 hence it can take the maximum value. While Epsilon Greedy is helpful when there is noisy rewards (takes more exploration) and also good for non-stationary rewards (like reward for certain action wont remain same every time, eg. playing chess, same move as before may not result in the same reward as the opponent might have made another move.

2. UCB (upper confidence bound action): another method to balance exploration vs exploitation

3. Gradient  Bandit algorithms: another method to balance exploration vs exploitation

4. Contextual Bandits: State transition problem


-x-x-x--x--x-x-x-x-x--x-x-x-x--x-x-x-x-x--x-x-xx--x-xx--xx--x-x-x-x-x-x--x-x--x-x

Chapter 3:




-x-x-x--x--x-x-x-x-x--x-x-x-x--x-x-x-x-x--x-x-xx--x-xx--xx--x-x-x-x-x-x--x-x--x-x
28.10.2022 

Chapter 4: Dynamic programming

exericse: 3.5 bellman equation for states s7 and s25.

Bellman eqn: an agent always prefers high state values.
expected total reward, combiation of all the probab. of things which can happen. 

V∗(s)=∑aπ(a|s).∑s'P(s'|a)[E(r|s,as')+γV∗(s')

here P is the deterministic probability

for state s7 and radom policy π, with gamma(discount) = 0.9
note: if we fall of the grid, reward is -1. We stay on grid, reward 0. 

Vπ* (s7) = 0.25 (0+0.9 * 1.5)  {left} + 0.25 (0+0.9*8.8) {top} + 0.25 (0+ 0.9 * 2.3) {right} + 0.25(0 + 0.9 * 0.7) {down}

where:
7th block is starting point -> from the current state, the agent is checking where to go next, which state to go next
0.25 is the prob. of 1/4 prob 

now for s25.
Vπ (s25) =  0.25 (0+0.9 * -1.4) +  0.25 (0+0.9 * -1.2) +  0.25 (-1+0.9 * v(25)) +  0.25 (-1+0.9 * v(25) = 0.25 (0+0.9 * -1.4) +  0.25 (0+0.9 * -1.2) +  0.25 (-1+0.9 * -2) +  0.25 (-1+0.9 * -2) =  -2.0 

note: if we fall of the board, -1 is the reward and discount comes with the same state. hence v25

what could we do to improve our random running? 
-> we will use dynamic programming. comparing 4 actions adn then running randomly. [action - value:: valeu of taking action a]
like 
q[13, left] = r + gamma* v(s12) = 0 + 0.9 * 0.7 = 0.63
q[13, top] = r + gamma* v(s8) = 0 + 0.9 * 2.3 = 2.07
q[13, right] = 0 + 0.9 * 0.4 = 0.36
q[13, bottom]  = 0 + 0.9 * -0.4 = - 0.36

in dynamic, we modify random policy pie, to take the best action, change action for s13 to top . choose a better action for each step.

Dynamic: [evaluating and iteration]

Dynamic Programming is a method (or better a set of varying methods) which is used to find optimal policies for finite Markov decision problems.
[Markov Decision Process: no memory, moving an agent inside the grid -> only current state, previous states doesnt matter]
The method is based on repeatedly running policy evaluation and policy improvement steps until a optimal (or near optimal) policy is found.

This process is called policy iteration



policy (pie) - evaluate - new value of the state - iterate the the new policy (pie1) - converges finally to the optimal policy resulting in optimal value of the state.

{look at the ss sent by kirtan or take notes from jaydip. I know I know, just get stuff done. Monkey}

-x-x-x-x-x-x-x-x-x-x-x-x--x--x-x-x-x-x-x-x-x-x-x-xx-x-x-x-x--x-x-x-x-x-x-xx-x--x-x-xx-x-x-x-x-x-x-x-x-x-x-x

04.11.2022 

[ left one hour of the class because of the failed interview, please catch up]

x-x-x-xx-x-x-x-x-x-x-x-x-x-x-x-x-x-
18.11.2022

> black jack: 
